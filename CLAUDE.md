# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

GPU-accelerated metagenomic classifier for **Jetson Nano (2GB) clusters**, based on CLARK 1.1.3. Three-layer architecture:

1. **CuCLARK** (`src/`) — CUDA C++ core that builds a k-mer hash-table database and classifies FASTQ reads on GPU. Produces `cuCLARK` (full) and `cuCLARK-l` (light/Jetson-optimized).
2. **arda** (`app/arda.cpp`) — Single-node C++11 orchestrator. Wraps shell scripts in `scripts/` to set up databases, classify, estimate abundance, and generate reports. CLI flags: `--verify`, `-d`, `-c`, `-a`, `-r`, `-h`.
3. **arda-mpi** (`app/arda_mpi.cpp`) — MPI cluster coordinator. Self-invokes via `mpirun`, broadcasts YAML config to workers, each worker runs `arda -c` locally, results aggregate on rank 0.

## Installation

### Quick Start

```bash
./install.sh                          # Bootstrap installation (checks environment, builds all)
./bin/arda --verify                   # Verify installation is complete
```

The `install.sh` script at the project root:
- Checks for required tools (g++, make, nvcc, mpicxx)
- Creates directory structure (bin/, logs/, results/, config/, data/)
- Builds components based on available tools:
  - If CUDA available: `make -C build all` (builds cuCLARK + arda)
  - If no CUDA: `make -C build arda` only (orchestrator still useful without GPU)
  - If MPI available: `make -C build arda-mpi` (optional cluster support)
- Verifies binaries were created successfully
- Writes installation marker to `logs/ardacpp_log.txt`

### Manual Build (Developers)

`app/Makefile` orchestrates all compilation; `src/Makefile` handles the CUDA core.

```
make -C build              # Build cuCLARK core + arda → bin/
make -C build arda         # Build single-node orchestrator only
make -C build arda-mpi     # Build MPI cluster coordinator (requires mpicxx/OpenMPI)
make -C build full         # Build all (cuCLARK + arda + arda-mpi)
make -C build clean        # Remove bin/ and src build artifacts
make -C src debug          # Debug build with TIME_DBLOADING, DEBUG_DMEM tracing
```

All binaries output to `bin/`. CUDA target arch is **sm_53** (Jetson Nano) — change `NVCCFLAGS` in `src/Makefile` for other GPUs.

`cuCLARK-l` is built by **swapping** `parameters.hh` with `parameters_light_hh` during compilation (smaller `HTSIZE`, lower `RESERVED` GPU memory). Do not edit this swap mechanism without understanding both parameter files.

## Running

### Single-Node Workflow

```bash
./bin/arda -h                                               # Show help and usage
./bin/arda --verify                                         # Verify installation status
./bin/arda -d <database_path>                               # Setup database targets (required before classify)
./bin/arda -c -O <fastq_file> -R <result_file> [options]   # Classify reads (default batch=32)
./bin/arda -a <database_path> <result_file>                 # Estimate abundance
./bin/arda -r                                               # Generate report
```

**Note:** The `-i` flag is now an alias for `--verify` and performs read-only verification (no builds). To app/rebuild, use `./install.sh` or `make -C build`.

### Cluster Mode (MPI)

```bash
./bin/arda-mpi -c config/cluster.conf -p             # Preflight checks (SSH, MPI)
./bin/arda-mpi -c config/cluster.conf                # Run cluster classification
./bin/arda-mpi -c config/cluster.conf -v             # Verbose mode
```

## Testing & Debugging

No automated test suite. Validate via example datasets referenced in `data/README.md`. Debug build enables `DEBUG_DMEM`, `TIME_DBLOADING`, and other trace macros. MPI diagnostics use `[MPI DIAG]` prefix on stderr.

## Key Conventions

- **Shell scripts in `scripts/` expect to be run FROM the `scripts/` directory** — they reference relative `.settings` and `../bin/`. The C++ orchestrators `cd scripts` before calling them.
- `.settings` file is auto-generated by `set_targets.sh`. Classification fails without it — always run `arda -d` first.
- Config uses a simple YAML-like format parsed by custom `YamlParser` in `arda_mpi.cpp` (only 0/2/4-space indent, `key: value`, `- item` lists).
- MPI data transfer uses `|`-delimited string serialization in `NodeResult::serialize/deserialize`. Keep this format stable.
- All binaries go to `bin/`, results to `results/`, logs to `logs/`. Never put outputs in root.

## Source Code Map

| Path | Role |
|---|---|
| `src/CuClarkDB.cu` | CUDA kernels: query, merge, result. GPU memory management. |
| `src/CuCLARK_hh.hh` | Main template class `CuCLARK<HKMERr>` — DB loading, classification pipeline. |
| `src/dataType.hh` | Core types: `IKMER`, `ILBL`, `RESULTS`, `CONTAINER`. K-mer encoding logic. |
| `src/parameters.hh` | Full-mode GPU constants (`HTSIZE`, `DBPARTSPERDEVICE`, `RESERVED`). |
| `src/parameters_light_hh` | Light-mode GPU constants (Jetson-optimized values). |
| `src/hashTable_hh.hh` / `HashTableStorage_hh.hh` | Host-side hash table for DB construction. |
| `app/arda.cpp` | Single-node CLI orchestrator (C++11 stdlib only, no CUDA/MPI). |
| `app/arda_mpi.cpp` | MPI coordinator — config parsing, MPI broadcast, result aggregation. |
| `app/Makefile` | Top-level build orchestrator; delegates CUDA builds to `src/Makefile`. |

## Working with CUDA Code

- Error checking uses `CUERR` / `CUMEMERR` macros in `CuClarkDB.cu`. Always use them after CUDA calls.
- GPU memory is pre-reserved via `RESERVED` constant. If out of GPU memory, increase batch count (`-b` flag), not `RESERVED`.
- Templates parameterized on `HKMERr` (k-mer representation type); instantiations in `CuCLARK_hh.hh`.

## Database Structure

The database directory (set via `arda -d <path>`) must contain:
- `Custom/` — FASTA reference files (`.fa`, `.fna`, `.fasta`)
- `taxonomy/` — NCBI taxonomy dumps (`names.dmp`, `nodes.dmp`, `nucl_accss`, etc.)
- `.taxondata` — auto-created marker file

## Dependencies

- NVIDIA CUDA Toolkit (sm_53 for Jetson Nano)
- g++ with C++11 support, GNU Make
- OpenMPI 4.0+ (only for arda-mpi)
- Passwordless SSH between cluster nodes (for MPI mode)
