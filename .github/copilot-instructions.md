# Copilot Instructions — jn_cuclark

## Project Overview

GPU-accelerated metagenomic classifier for **Jetson Nano clusters**. Three-layer architecture:

1. **CuCLARK** (`src/`) — CUDA C++ core that builds a k-mer hash-table database and classifies FASTQ reads on GPU. Produces `cuCLARK` (full) and `cuCLARK-l` (light/Jetson). Based on CLARK 1.1.3 by Rachid Ounit.
2. **arda** (`arda.cpp`) — Single-node C++ orchestrator. Wraps shell scripts (`scripts/`) to install, set up databases, classify, estimate abundance, and generate reports. CLI: `-i`, `-d`, `-c`, `-a`, `-r`.
3. **arda-mpi** (`arda_mpi.cpp`) — MPI cluster coordinator. Self-invokes via `mpirun`, broadcasts config (YAML in `config/cluster.conf`) to workers, each worker runs `arda -c` locally, results aggregate on rank 0.

## Build System

Two-level Makefile. Root `Makefile` delegates CUDA builds to `src/Makefile`.

```
make            # build cuCLARK core + arda → bin/
make arda-mpi   # requires mpicxx (OpenMPI)
make full       # all of the above
make clean      # removes bin/ and src build artifacts
```

- CUDA target arch is **sm_53** (Jetson Nano). Change `NVCCFLAGS` in `src/Makefile` for other GPUs.
- `cuCLARK-l` is built by **swapping** `parameters.hh` with `parameters_light_hh` during compilation (smaller hash table `HTSIZE`, lower `RESERVED` GPU memory). Do not edit this swap mechanism without understanding both parameter files.
- Debug flags (kernel/memory tracing): `make debug` in `src/`.
- `arda.cpp` uses only C++11 stdlib (no CUDA, no MPI). `arda_mpi.cpp` requires MPI + pthreads.

## Key Conventions

- **All binaries go to `bin/`**, results to `results/`, logs to `logs/`. Never put outputs in root.
- Shell scripts in `scripts/` expect to be run from the `scripts/` directory (they reference relative `.settings` and `../bin/`). The C++ orchestrators `cd scripts` before calling them.
- `scripts/classify_metagenome.sh` reads a `.settings` file generated by `set_targets.sh`. Classification will fail without it — always run database setup (`arda -d`) first.
- Config uses a simple YAML-like format parsed by a custom `YamlParser` class in `arda_mpi.cpp` (not a full YAML parser; only 0/2/4-space indent, `key: value`, and `- item` lists).
- MPI data transfer uses string serialization (`NodeResult::serialize/deserialize`) with `|`-delimited fields. Keep this format stable.

## Source Code Map

| Path | Role |
|---|---|
| `src/CuClarkDB.cu` | CUDA kernels: query, merge, result. GPU memory management. |
| `src/CuCLARK_hh.hh` | Main template class `CuCLARK<HKMERr>` — DB loading, classification pipeline. |
| `src/dataType.hh` | Core types: `IKMER`, `ILBL`, `RESULTS`, `CONTAINER`. K-mer encoding logic. |
| `src/parameters.hh` | Full-mode constants (`HTSIZE=1610612741`, `DBPARTSPERDEVICE=3`). |
| `src/parameters_light_hh` | Light-mode constants (`HTSIZE=57777779`, `DBPARTSPERDEVICE=1`). |
| `src/hashTable_hh.hh` / `HashTableStorage_hh.hh` | Host-side hash table for DB construction. |
| `src/analyser.cc` | K-mer frequency analysis (bump interval detection). |
| `arda.cpp` | Single-node CLI orchestrator. |
| `arda_mpi.cpp` | MPI coordinator — config parsing, MPI broadcast, result aggregation, hostfile generation. |

## Working with CUDA Code

- Error checking uses `CUERR` / `CUMEMERR` macros in `CuClarkDB.cu`. Always use them after CUDA calls.
- GPU memory is pre-reserved (`RESERVED` constant in parameters). If out of GPU memory, increase batch count (`-b` flag), not `RESERVED`.
- Templates are parameterized on `HKMERr` (k-mer representation type). All template instantiations happen in `CuCLARK_hh.hh`.

## Database Structure

The database directory (set via `arda -d <path>`) must contain:
- `Custom/` — FASTA reference files (`.fa`, `.fna`, `.fasta`)
- `taxonomy/` — NCBI taxonomy dumps (`names.dmp`, `nodes.dmp`, `nucl_accss`, etc.)
- `.taxondata` — auto-created marker file

## Testing & Debugging

- No automated test suite. Validate via the example dataset from `data/README.md`.
- `make debug` in `src/` enables `DEBUG_DMEM`, `TIME_DBLOADING` and other trace macros.
- MPI diagnostics: `arda-mpi -c config/cluster.conf -p` runs preflight checks (SSH, binary presence, MPI connectivity).
- Each MPI process prints `[MPI DIAG]` on stderr at startup for troubleshooting rank/host mapping.
